# Network Latency Measurement Training Config
#
# This config trains a decoder-only transformer on network ping measurements.
# Dataset: 100M rows of RIPE Atlas measurements with IPv4/IPv6 addresses and RTT data
#
# Usage:
#   DECOUPLE_GCLOUD=TRUE python -m MaxText.train src/MaxText/configs/latency_network.yml \
#     run_name=latency_test steps=5

# Inherit from decoupled base for local/CPU testing
base_config: decoupled_base_test.yml

# Run configuration
run_name: latency_network_training
base_output_directory: outputs/latency_network

# Model architecture - PLAN_2: ~95M param decoder-only transformer
# Optimized for generalization (deep + narrow MLP ratio)
vocab_size: 267  # 11 role tokens + 256 byte tokens (PLAN_2)
base_num_decoder_layers: 20  # Deep for multi-step reasoning (inverse search)
base_emb_dim: 640  # Optimized for small vocab
base_num_query_heads: 10  # 640 / 64 = 10 heads
base_num_kv_heads: 10  # Standard attention (not MQA/GQA)
head_dim: 64  # Standard head dimension
base_mlp_dim: 2048  # 3.2x ratio (vs standard 4x) for generalization
max_target_length: 1024
max_prefill_predict_length: 1024

# Training - PLAN_2 hyperparameters
steps: 200000
per_device_batch_size: 32  # Batch size for single A100 (PLAN_2)
learning_rate: 3.0e-4  # Standard for small-medium models
learning_rate_schedule: "cosine"
warmup_steps: 2000
adam_b1: 0.9
adam_b2: 0.999
adam_eps: 1.0e-8
adam_eps_root: 0.0
weight_decay: 0.01
dropout_rate: 0.1

# Checkpointing
enable_checkpointing: true
checkpoint_period: 5000

# Logging & Monitoring
log_period: 100           # Log metrics every 100 steps
eval_interval: 1000       # Evaluate every 1000 steps
eval_steps: 100           # Run 100 eval steps
enable_tensorboard: true  # Enable TensorBoard logging
tensorboard_dir: ""       # Auto-set to base_output_directory/tensorboard

# Dataset
# Use Grain pipeline with ArrayRecord network measurements (fast random access)
dataset_type: "grain"
grain_file_type: "network_arrayrecord"  # Custom network datasource for ArrayRecord
grain_train_files: "data/arrayrecord/train.arrayrecord"
grain_eval_files: "data/arrayrecord/test.arrayrecord"
grain_worker_count: 16  # Use 64 CPU cores - overfeed the GPU! (thread-safety fixed!)

# Attention & Positional Encoding
attention: "autoselected"  # Auto-selects best attention for hardware (flash on GPU, falls back if needed)
scan_layers: true
position_embedding: "rope"  # RoPE for relative position encoding (PLAN_2)

# Precision
dtype: "bfloat16"

# GPU Optimizations
use_iota_embed: true  # Memory-efficient embeddings
remat_policy: "minimal_with_context"  # Optimized for small models (less recomputation than "full")

# Hardware (override via CLI: hardware=cpu or hardware=gpu)
hardware: "gpu"  # Set to GPU for production training

# Skip distributed JAX setup for single-process local testing
skip_jax_distributed_system: true
