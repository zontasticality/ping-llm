# Network Latency Measurement Training Config
#
# This config trains a decoder-only transformer on network ping measurements.
# Dataset: 100M rows of RIPE Atlas measurements with IPv4/IPv6 addresses and RTT data
#
# Usage:
#   DECOUPLE_GCLOUD=TRUE python -m MaxText.train src/MaxText/configs/latency_network.yml \
#     run_name=latency_test steps=5

# Inherit from decoupled base for local/CPU testing
base_config: decoupled_base_test.yml

# Run configuration
run_name: latency_network_training
base_output_directory: outputs/latency_network

# Model architecture - PLAN_2: ~95M param decoder-only transformer
# Optimized for generalization (deep + narrow MLP ratio)
vocab_size: 267  # 11 role tokens + 256 byte tokens (PLAN_2)
base_num_decoder_layers: 20  # Deep for multi-step reasoning (inverse search)
base_emb_dim: 640  # Optimized for small vocab
base_num_query_heads: 10  # 640 / 64 = 10 heads
base_num_kv_heads: 10  # Standard attention (not MQA/GQA)
head_dim: 64  # Standard head dimension
base_mlp_dim: 2048  # 3.2x ratio (vs standard 4x) for generalization
max_target_length: 1024
max_prefill_predict_length: 1024

# Training - PLAN_2 hyperparameters
steps: 200000
per_device_batch_size: 128  # Increased from 32 to utilize A100 (70% GPU memory was idle)
learning_rate: 6.0e-4  # Scaled up 2x for 4x larger batch (sqrt scaling rule)
learning_rate_schedule: "cosine"
warmup_steps: 2000
adam_b1: 0.9
adam_b2: 0.999
adam_eps: 1.0e-8
adam_eps_root: 0.0
weight_decay: 0.01
dropout_rate: 0.1

# Checkpointing
enable_checkpointing: true
checkpoint_period: 5000

# Logging & Monitoring
log_period: 100           # Log metrics every 100 steps
eval_interval: 1000       # Evaluate every 1000 steps
eval_steps: 100           # Run 100 eval steps
enable_tensorboard: true  # Enable TensorBoard logging
tensorboard_dir: ""       # Auto-set to base_output_directory/tensorboard

# Dataset
# Using Grain pipeline with sharded parquet files (200M rows, 180 train + 20 test shards)
# Note: SLURM allocates 16 CPU cores + 60GB RAM. Data (~1.8GB) is eagerly loaded into memory
# at startup, eliminating disk I/O during training.
dataset_type: "grain"
packing: False  # Disable packing to avoid SequenceDescriptor incompatibility with cudnn_flash_te
grain_file_type: "network_parquet"  # Use custom network measurement data source
grain_train_files: "data/sharded/train/*.parquet"
grain_eval_files: "data/sharded/test/*.parquet"
grain_worker_count: 12  # Use 12 workers (out of 16 cores, leaving 4 for GPU/system)
grain_per_worker_buffer_size: 32  # Large buffer per worker for maximum throughput (12 workers * 32 = 384 batches buffered)

# Attention & Positional Encoding
attention: "cudnn_flash_te"
scan_layers: true
position_embedding: "rope"  # RoPE for relative position encoding (PLAN_2)

# Precision
dtype: "bfloat16"

# Hardware (override via CLI: hardware=cpu or hardware=gpu)
hardware: "cpu"

# Skip distributed JAX setup for single-process local testing
skip_jax_distributed_system: true
